{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdaa8dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from langdetect import DetectorFactory\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "import googletrans\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f982e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "cosmeticScoreDS = pd.read_csv('trainDS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb56ca1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'af': 'afrikaans', 'sq': 'albanian', 'am': 'amharic', 'ar': 'arabic', 'hy': 'armenian', 'az': 'azerbaijani', 'eu': 'basque', 'be': 'belarusian', 'bn': 'bengali', 'bs': 'bosnian', 'bg': 'bulgarian', 'ca': 'catalan', 'ceb': 'cebuano', 'ny': 'chichewa', 'zh-cn': 'chinese (simplified)', 'zh-tw': 'chinese (traditional)', 'co': 'corsican', 'hr': 'croatian', 'cs': 'czech', 'da': 'danish', 'nl': 'dutch', 'en': 'english', 'eo': 'esperanto', 'et': 'estonian', 'tl': 'filipino', 'fi': 'finnish', 'fr': 'french', 'fy': 'frisian', 'gl': 'galician', 'ka': 'georgian', 'de': 'german', 'el': 'greek', 'gu': 'gujarati', 'ht': 'haitian creole', 'ha': 'hausa', 'haw': 'hawaiian', 'iw': 'hebrew', 'he': 'hebrew', 'hi': 'hindi', 'hmn': 'hmong', 'hu': 'hungarian', 'is': 'icelandic', 'ig': 'igbo', 'id': 'indonesian', 'ga': 'irish', 'it': 'italian', 'ja': 'japanese', 'jw': 'javanese', 'kn': 'kannada', 'kk': 'kazakh', 'km': 'khmer', 'ko': 'korean', 'ku': 'kurdish (kurmanji)', 'ky': 'kyrgyz', 'lo': 'lao', 'la': 'latin', 'lv': 'latvian', 'lt': 'lithuanian', 'lb': 'luxembourgish', 'mk': 'macedonian', 'mg': 'malagasy', 'ms': 'malay', 'ml': 'malayalam', 'mt': 'maltese', 'mi': 'maori', 'mr': 'marathi', 'mn': 'mongolian', 'my': 'myanmar (burmese)', 'ne': 'nepali', 'no': 'norwegian', 'or': 'odia', 'ps': 'pashto', 'fa': 'persian', 'pl': 'polish', 'pt': 'portuguese', 'pa': 'punjabi', 'ro': 'romanian', 'ru': 'russian', 'sm': 'samoan', 'gd': 'scots gaelic', 'sr': 'serbian', 'st': 'sesotho', 'sn': 'shona', 'sd': 'sindhi', 'si': 'sinhala', 'sk': 'slovak', 'sl': 'slovenian', 'so': 'somali', 'es': 'spanish', 'su': 'sundanese', 'sw': 'swahili', 'sv': 'swedish', 'tg': 'tajik', 'ta': 'tamil', 'te': 'telugu', 'th': 'thai', 'tr': 'turkish', 'uk': 'ukrainian', 'ur': 'urdu', 'ug': 'uyghur', 'uz': 'uzbek', 'vi': 'vietnamese', 'cy': 'welsh', 'xh': 'xhosa', 'yi': 'yiddish', 'yo': 'yoruba', 'zu': 'zulu'}\n"
     ]
    }
   ],
   "source": [
    "print(googletrans.LANGUAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05379aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting only usefull data from import dataset\n",
    "cosmeticScoreDS = cosmeticScoreDS.iloc[:,[4,16,14,11]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91d749d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify missing data\n",
    "data = pd.DataFrame(cosmeticScoreDS)\n",
    "cosmeticScoreDS = data.dropna(axis = 0,how='any')\n",
    "X = cosmeticScoreDS.iloc[:,:-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d361af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stored data which of rating field\n",
    "rating = X[:,-1]\n",
    "Y = cosmeticScoreDS.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "887433e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4458b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contour</td>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Foundation</td>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Super Food</td>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lip Stain</td>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acne Treatment</td>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>Foundation</td>\n",
       "      <td>Nice upgrade from my old kindle. I like it ver...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>Super Food</td>\n",
       "      <td>For the price this is perfect. I read books, s...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Lip Stain</td>\n",
       "      <td>İt is cheap tablet option coming from amazon. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Acne Treatment</td>\n",
       "      <td>Very descent for a non-IPAD. Good speakers. Gr...</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>Nivea</td>\n",
       "      <td>This thing looks nice, runs nice and feels nic...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>598 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0                                                  1  2  \\\n",
       "0           Contour  This product so far has not disappointed. My c...  5   \n",
       "1        Foundation  great for beginner or experienced person. Boug...  5   \n",
       "2        Super Food  Inexpensive tablet for him to use and learn on...  5   \n",
       "3         Lip Stain  I've had my Fire HD 8 two weeks now and I love...  4   \n",
       "4    Acne Treatment  I bought this for my grand daughter when she c...  5   \n",
       "..              ...                                                ... ..   \n",
       "593      Foundation  Nice upgrade from my old kindle. I like it ver...  5   \n",
       "594      Super Food  For the price this is perfect. I read books, s...  5   \n",
       "595       Lip Stain  İt is cheap tablet option coming from amazon. ...  5   \n",
       "596  Acne Treatment  Very descent for a non-IPAD. Good speakers. Gr...  4   \n",
       "597           Nivea  This thing looks nice, runs nice and feels nic...  5   \n",
       "\n",
       "        3  \n",
       "0    True  \n",
       "1    True  \n",
       "2    True  \n",
       "3    True  \n",
       "4    True  \n",
       "..    ...  \n",
       "593  True  \n",
       "594  True  \n",
       "595  True  \n",
       "596  True  \n",
       "597  True  \n",
       "\n",
       "[598 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data visualization\n",
    "cosmeticScoreDS.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec294a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Contour'\n",
      "  'This product so far has not disappointed. My children love to use it and I like the ability to monitor control what content they see with ease.'\n",
      "  5]\n",
      " ['Foundation'\n",
      "  'great for beginner or experienced person. Bought as a gift and she loves it'\n",
      "  5]\n",
      " ['Super Food'\n",
      "  'Inexpensive tablet for him to use and learn on, step up from the NABI. He was thrilled with it, learn how to Skype on it already...'\n",
      "  5]\n",
      " ...\n",
      " ['Acne Treatment'\n",
      "  'Very descent for a non-IPAD. Good speakers. Great price. The screen is a little dark. Great buy if you have Amazon Prime since you can download stuff and watch it offline (great for trips, etc...). Overall great cheap IPAD or even as a back-up tablet. Terrific battery life too.'\n",
      "  4]\n",
      " ['Nivea'\n",
      "  \"This thing looks nice, runs nice and feels nice and for the price I don't think you can find a better tablet.\"\n",
      "  5]\n",
      " ['Face Wash'\n",
      "  \"Got this for my 9 year old after her ipad crashed and burned. She does not like androids at all but I couldn't afford a expensive tablet for her. So we took a chance with this one. She is pleased and I'm pleased it seems so sturdy , not cheap quality at all. Going to get our youngest child one now. Very pleased.\"\n",
      "  4]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#visualize type of X and Y\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49107b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean dataset\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#reduce words in their root form\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "all_stopwords = stopwords.words('english')\n",
    "\n",
    "# Removing some stopwords which have significance effect in building this model\n",
    "rem = [ 'just', 'too', 'very', 'no', 'nor', 'only', 'own', 'same', 'again', 'against', 'but', 'not', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \n",
    "       \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \n",
    "       \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'don', \"don't\",]\n",
    "for s in rem:\n",
    "  all_stopwords.remove(s)\n",
    "\n",
    "def find_clean_text(temp):\n",
    "  temp = re.sub('[^a-zA-Z]', ' ', temp)\n",
    "  temp = temp.lower()\n",
    "  temp = temp.split()\n",
    "  temp = [ps.stem(word) for word in temp if not word in set(all_stopwords)]\n",
    "  temp = ' '.join(temp)\n",
    "  return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0f86205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatanating both title and detailed review\n",
    "corpus = []\n",
    "for i in range(X.shape[0]):\n",
    "  temp = X[i][0] + ' ' + X[i][1]\n",
    "  temp = find_clean_text(temp)\n",
    "  corpus.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b5c00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bag of word model\n",
    "cv = CountVectorizer(max_features = 3000)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "513abfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding rating in the matrix of feature X\n",
    "rating = rating.reshape(rating.shape[0],1)\n",
    "X = np.append(X,rating,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "978f02bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into test set and train set\n",
    "pos_x = []\n",
    "pos_y = []\n",
    "neg_x = []\n",
    "neg_y = []\n",
    "for i in range(X.shape[0]):\n",
    "  if Y[i]==1:\n",
    "    pos_x.append(X[i])\n",
    "    pos_y.append(Y[i])\n",
    "  else:\n",
    "    neg_x.append(X[i])\n",
    "    neg_y.append(Y[i])\n",
    "\n",
    "X_train1, X_test1, Y_train1, Y_test1 = train_test_split(pos_x, pos_y, test_size = 0.20)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(neg_x, neg_y, test_size = 0.20)\n",
    "\n",
    "for i in range(len(X_train1)):\n",
    "  X_train.append(X_train1[i])\n",
    "  Y_train.append(Y_train1[i])\n",
    "for i in range(len(X_test1)):\n",
    "  X_test.append(X_test1[i])\n",
    "  Y_test.append(Y_test1[i])\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34752868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the multinomial naive bayes(MNB) model on the Training set\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0dc865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on training set :\n",
      "Confusion matrix :\n",
      "[[  3   9]\n",
      " [  0 467]]\n",
      "accuracy :  0.9812108559498957\n",
      "Result on test set :\n",
      "Confusion matrix :\n",
      "[[  0   3]\n",
      " [  0 117]]\n",
      "accuracy 0.975\n"
     ]
    }
   ],
   "source": [
    "#Making the Confusion Matrix  ---using SciKit-learn metrics\n",
    "print('Result on training set :')\n",
    "print('Confusion matrix :')\n",
    "print(confusion_matrix(Y_train,classifier.predict(X_train)))\n",
    "print('accuracy : ',accuracy_score(Y_train, classifier.predict(X_train)))\n",
    "\n",
    "print('Result on test set :')\n",
    "Y_pred = classifier.predict(X_test)\n",
    "print('Confusion matrix :')\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print('accuracy',accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c0c0dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Blush \n",
      "Review : This is my first Amazon e-reader-tablet and the second e-reader I have. It is easy to use once you get through the initial hoops. I like the free games and the fact that I new can load books from my computer to the tablet. And one thing, I am not in the market for a tablet but for a bigger e-reader. It is a lot better than the Nook that I have. I love this product \n",
      "Rating : 5\n",
      "True value : True\n",
      "Prediction : [1]\n"
     ]
    }
   ],
   "source": [
    "#Making Prediction on some reviews from test set\n",
    "random.seed(13)\n",
    "translator = Translator()\n",
    "\n",
    "index = random.randrange(cosmeticScoreDS.shape[0])\n",
    "result = translator.translate(cosmeticScoreDS[1][index])\n",
    "print('Title :',cosmeticScoreDS[0][index],'\\nReview :',result.text,'\\nRating :',cosmeticScoreDS[2][index])\n",
    "print(\"True value :\", cosmeticScoreDS[3][index])\n",
    "print(\"Prediction :\",classifier.predict(np.append(cv.transform([find_clean_text(cosmeticScoreDS[0][index]+' '+cosmeticScoreDS[1][index])]).toarray(),[[cosmeticScoreDS[3][index]]],axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d669f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating the accuracy\n",
    "accuracy_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16a22d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.svm import SVC \n",
    "svm_model_pkl = open('PScoreRating_model.pkl', 'wb')\n",
    "pickle.dump(cosmeticScoreDS, svm_model_pkl)\n",
    "svm_model_pkl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56e680f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmeticScoreDS.to_csv('pscorerating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c38d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
